---
template: post
title: Spark on Kubernetes táº¡i Fossil ğŸ¤”
date: "2022-03-09"
author: Van-Duyet Le
category: Data
tags:
  - Vietnamese
  - Data Engineer
  - Spark
  - Kubernetes
slug: /2022/03/spark-kubernetes-at-fossil.html
thumbnail: https://blogger.googleusercontent.com/img/a/AVvXsEggpb4U-cWkhLQo1R-OUORtAvLFPnn0LB22LJ9mOBSpWiC3yoqM3iOoo1BlilS5nxhnOmNs8JyUsVHFdA4dKOLGqRH9WoBXAJxn8v-cg18QFuJFbPHwg_5V6N_0gtgtRpy41fCLICGakuAayr9p5Bwlr02rrmDVjYxOBU4hwL6Oz4gWWXs0VFYDQK-lFw
draft: false
fbCommentUrl: none
twitterCommentUrl: https://twitter.com/search?q=https%3A%2F%2Fblog.duyet.net%2F2022%2F03%2Fspark-kubernetes-at-fossil.html
linkedInCommentUrl: https://www.linkedin.com/posts/duyet_spark-on-kubernetes-t%E1%BA%A1i-fossil-activity-6908001211849555969-j5Ss
description: Apache Spark Ä‘Æ°á»£c chá»n lÃ m cÃ´ng nghá»‡ cho Batch layer bá»Ÿi kháº£ nÄƒng xá»­ lÃ½ má»™t lÆ°á»£ng lá»›n data cÃ¹ng má»™t lÃºc. á» thiáº¿t káº¿ ban Ä‘áº§u, team data chá»n sá»­ dá»¥ng Apache Spark trÃªn AWS EMR do cÃ³ sáºµn vÃ  triá»ƒn khai nhanh chÃ³ng. Dáº§n dáº§n, AWS EMR bá»™c lá»™ má»™t sá»‘ Ä‘iá»ƒm háº¡n cháº¿ trÃªn mÃ´i trÆ°á»ng Production. Trong bÃ i viáº¿t nÃ y, mÃ¬nh sáº½ nÃ³i vá» táº¡i sao vÃ  lÃ m tháº¿ nÃ o team Data chuyá»ƒn tá»« Spark trÃªn AWS EMR sang Kubernetes.

---

Táº¡i [Fossil](https://sites.google.com/fossil.com/fossil-vietnam/home), 
cÃ³ hÃ ng trÄƒm triá»‡u log records Ä‘Æ°á»£c thu tháº­p má»—i ngÃ y, 
Ä‘Æ°á»£c xá»­ lÃ½ vÃ  lÆ°u trá»¯ trong cÃ¡c Data Warehouse bá»Ÿi há»‡ thá»‘ng **Fossil Data Platform**. 
Data Platform lÃ  má»™t há»‡ thá»‘ng event-driven Ä‘Æ°á»£c thiáº¿t káº¿ dá»±a trÃªn Lambda Architecture 
gá»“m má»™t near-realtime layer vÃ  má»™t batch layer. Near-realtime layer cho phÃ©p data tá»« 
lÃºc Ä‘áº©y vÃ o há»‡ thá»‘ng cho Ä‘áº¿n khi xuáº¥t hiá»‡n á»Ÿ Ä‘áº§u cuá»‘i cÃ³ Ä‘á»™ trá»… tá»‘i Ä‘a 15 phÃºt. 
Batch layer sáº½ tÃ­nh toÃ¡n bá»™ data láº¡i má»™t láº§n ná»¯a, vÃ o cuá»‘i má»—i ngÃ y, Ä‘á»ƒ Ä‘áº£m báº£o data 
Ä‘Æ°á»£c chÃ­nh xÃ¡c vÃ  tá»‘i Æ°u hÃ³a Ä‘á»ƒ lÆ°u trá»¯ lÃ¢u dÃ i.

Há»‡ thá»‘ng Ä‘Æ°á»£c triá»ƒn khai trÃªn Kubernetes Cluster bao gá»“m nhiá»u thÃ nh pháº§n. 
Má»™t sá»‘ thÃ nh pháº§n cÃ³ thá»ƒ ká»ƒ Ä‘áº¿n nhÆ°: *API Ingession*, [*CDC*](https://debezium.io), 
[*Kafka Connector*](https://docs.confluent.io/platform/current/connect/index.html), 
cÃ¡c *Parser* vÃ  *Transformer* xá»­ lÃ½ raw data. 
[**Apache Airflow**](https://airflow.apache.org/) vÃ  [**Apache Spark**](https://spark.apache.org/) 
cÅ©ng Ä‘Æ°á»£c triá»ƒn khai trÃªn [**Kubernetes**](https://kubernetes.io), quáº£n lÃ½ bá»Ÿi cÃ¡c 
[Kubernetes Operators](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/).

Apache Spark Ä‘Æ°á»£c chá»n lÃ m cÃ´ng nghá»‡ cho Batch layer bá»Ÿi kháº£ nÄƒng xá»­ lÃ½ 
má»™t lÆ°á»£ng lá»›n data cÃ¹ng má»™t lÃºc. á» thiáº¿t káº¿ ban Ä‘áº§u, team data chá»n sá»­ dá»¥ng 
Apache Spark trÃªn **AWS EMR** do cÃ³ sáºµn vÃ  triá»ƒn khai nhanh chÃ³ng. 
Dáº§n dáº§n, AWS EMR bá»™c lá»™ má»™t sá»‘ Ä‘iá»ƒm háº¡n cháº¿ trÃªn mÃ´i trÆ°á»ng Production.

Trong bÃ i viáº¿t nÃ y, mÃ¬nh sáº½ nÃ³i vá» táº¡i sao vÃ  lÃ m tháº¿ nÃ o team 
Data chuyá»ƒn tá»« Spark trÃªn AWS EMR sang Kubernetes.

<div class="toc">
  <p>Trong bÃ i nÃ y:</p>
  <ul>
    <li><a href="#1-apache-spark-trÃªn-aws-emr">1. Apache Spark trÃªn AWS EMR</a></li>
    <li><a href="#2-spark-on-kubernetes---livy">2. Spark on Kubernetes - Livy</a></li>
    <li>
      <a href="#3-spark-on-kubernetes---spark-operator">3. Spark on Kubernetes - Spark Operator</a>
      <ul>
        <li><a href="#31-spark-operator">3.1. Spark Operator</a></li>
        <li><a href="#32-spark-submit-worker">3.2. Spark Submit Worker</a></li>
        <li><a href="#33-spark-jobs-ui">3.3. Spark Jobs UI</a></li>
        <li><a href="#34-spark-history-server">3.4. Spark History Server</a></li>
      </ul>
    </li>
    <li><a href="#4-performance-tuning-on-kubernetes">4. Performance Tuning on Kubernetes</a></li>
    <li><a href="#5-káº¿t">5. Káº¿t</a></li>
    <li><a href="#6-references">6. References</a></li>
  </ul>
</div>

# 1. Apache Spark trÃªn AWS EMR

Trong tháº¿ giá»›i cá»§a Data Engineering thÃ¬ [**Apache Spark**](https://spark.apache.org/) khÃ´ng cÃ²n quÃ¡ xa láº¡. 
Spark lÃ  open source vá»›i má»¥c Ä‘Ã­ch triá»ƒn khai má»™t há»‡ thá»‘ng tÃ­nh toÃ¡n in-memory vÃ  massively parallel. 
Spark Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trong nhiá»u lÄ©nh vá»±c xá»­ lÃ½ Big Data, tá»« Data Analytics Ä‘áº¿n Machine Learning. 
Spark Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ cÃ³ thá»ƒ cháº¡y á»Ÿ Standalone Mode cÅ©ng nhÆ° trÃªn Mesos, YARN vÃ  Kubernetes.

á» thiáº¿t káº¿ Ä‘áº§u tiÃªn, team Fossil Data Platform thiáº¿t káº¿ sá»­ dá»¥ng Apache Spark 
Ä‘á»ƒ cháº¡y cÃ¡c Jobs cÃ¹ng vá»›i Apache Hive trÃªn AWS EMR. Äiá»u nÃ y háº¿t sá»©c Ä‘Æ¡n giáº£n 
do viá»‡c thiáº¿t láº­p cá»¥m AWS EMR khÃ¡ dá»… dÃ ng vÃ  nhanh chÃ³ng.
Dáº§n dáº§n sau má»™t khoáº£ng thá»i gian, team nháº­n ra cÃ³ má»™t sá»‘ Ä‘iá»ƒm yáº¿u:

- Táº¡i thá»i Ä‘iá»ƒm Ä‘Ã³ AWS chÆ°a ra máº¯t *EMR Serverless* vÃ  *EMR on EKS*, viá»‡c scale thÃªm EC2 Node tá»‘n thá»i gian do pháº£i bootstrap (cÃ i Ä‘áº·t vÃ  khá»Ÿi Ä‘á»™ng) 1 loáº¡t cÃ¡c services cáº§n thiáº¿t.
- TrÃªn má»—i Node sáº½ tá»‘n 1 pháº§n resources overhead Ä‘á»ƒ cháº¡y cÃ¡c services Ä‘Ã³ (Spark, Livy, Zeppelin, Hive, HDFS, Monitoring, â€¦).
- Chi phÃ­ quáº£n lÃ½ EMR Cluster.
- HA trÃªn EMR báº¯t buá»™c báº¡n pháº£i cÃ³ 3 node master cháº¡y song song, náº¿u 1 node master cháº¿t thÃ¬ node khÃ¡c lÃªn thay, nhÆ°ng bÃ¬nh thÆ°á»ng sáº½ lÃ£ng phÃ­ 2 node backup khÃ´ng lÃ m gÃ¬ cáº£.
- ...

Trong khi toÃ n bá»™ há»‡ thá»‘ng Data Platform Ä‘Æ°á»£c thiáº¿t káº¿ dÆ°á»›i dáº¡ng micro-services 
vÃ  *event-driven architecture* vá»›i nhiá»u thÃ nh pháº§n cháº¡y trÃªn Kubernetes, 
team báº¯t Ä‘áº§u nghÄ© Ä‘áº¿n viá»‡c deploy Spark Jobs trÃªn Kubernetes thay vÃ¬ EMR, cÃ³ má»™t sá»‘ Æ°u Ä‘iá»ƒm cÃ³ thá»ƒ ká»ƒ Ä‘áº¿n:

- Tiáº¿t kiá»‡m chi phÃ­, bao gá»“m chi phÃ­ cho viá»‡c Ä‘á»£i provisioning vÃ  bootstrapping phá»©c táº¡p, costing Ä‘Æ°á»£c tÃ­nh theo giÃ¢y, viá»‡c nÃ y cÅ©ng giÃºp loáº¡i bá» chi phÃ­ quáº£n lÃ½ EMR cluster, khoáº£ng **$700-$800** cho má»™t thÃ¡ng (chÆ°a bao gá»“m chi phÃ­ EC2).
- Spark trÃªn YARN cÅ©ng tá»‘n chi phÃ­ maintenance khÃ´ng nhá».
- Tiáº¿t kiá»‡m chi phÃ­ do khÃ´ng pháº£i duy trÃ¬ má»™t lÃºc 3 Node Master HA.
- KhÃ´ng thá»ƒ cháº¡y nhiá»u version cá»§a Spark khÃ¡c nhau, vÃ­ dá»¥ Ä‘ang sá»­ dá»¥ng Spark 2.4.x, báº¡n cáº§n upgrade má»™t sá»‘ Application lÃªn Spark 3.x Ä‘á»ƒ dÃ¹ng tÃ­nh nÄƒng má»›i, báº¯t buá»™c pháº£i upgrade cÃ¡c Application cÅ© hoáº·c cÃ i Ä‘áº·t má»™t Cluster EMR má»›i. NgÆ°á»£c láº¡i Spark trÃªn Kubernetes cho phÃ©p cháº¡y cÃ¡c driver, executer trÃªn cÃ¡c Kubernetes Pod, má»—i Pod gá»“m 1 container nÃªn cÃ³ thá»ƒ isolated workloads dá»… dÃ ng. NgoÃ i ra cÃ³ thá»ƒ thá»«a hÆ°á»Ÿng Ä‘Æ°á»£c má»i tÃ­nh nÄƒng cá»§a Kubernetes nhÆ°:
    - [Request/Limit](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/): Ä‘iá»u chá»‰nh hay giá»›i háº¡n resources (mem, cpu), sá»‘ lÆ°á»£ng Pod cho má»—i Spark Application.
    - [Namespace](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/): Kubernetes Namespace cÃ²n cho phÃ©p phÃ¢n quyá»n cho cÃ¡c team, cÃ¡c mÃ´i trÆ°á»ng vá»›i lÆ°á»£ng resources xÃ¡c Ä‘á»‹nh ná»¯a (e.g. namespace: `data-prod`, `data-stag`, `data-dev`, â€¦)
    - Táº­n dá»¥ng Ä‘Æ°á»£c [Kubernetes Autoscaler](https://github.com/kubernetes/autoscaler) vÃ  cÃ³ kháº£ nÄƒng scale-to-zero.
    - [Node Selector vÃ  Affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/): cho phÃ©p chá»n loáº¡i Node tÃ¹y theo tÃ­nh cháº¥t cá»§a Jobs Ä‘Ã³, vÃ­ dá»¥ má»™t sá»‘ Jobs cáº§n nhiá»u Mem, trong khi má»™t khá»‘ Jobs khÃ¡c cáº§n nhiá»u CPU.

# 2. Spark on Kubernetes - Livy

Ká»ƒ tá»« 2.3.x lÃ  Spark Ä‘Ã£ há»— trá»£ cháº¡y trÃªn cluster quáº£n lÃ½ bá»Ÿi Kubernetes. ChÃºng ta cÃ³ thá»ƒ submit má»™t Spark Application báº¥t ká»³ trá»±c tiáº¿p báº±ng cÃ¡ch sá»­ dá»¥ng `spark-submit` trÃªn comand line, chá»‰ cáº§n thay `--master` Ä‘áº¿n Ä‘á»‹a chá»‰ cá»§a Kubernetes `k8s://<api_server_host>:<k8s-apiserver-port>`

VÃ­ dá»¥ Ä‘á»ƒ cháº¡y Spark Pi trÃªn Cluster mode, hÃ£y xem vÃ­ dá»¥ sau:

```bash
$ bin/spark-submit \
   --master k8s://https://<k8s-apiserver-host>:<k8s-apiserver-port> \
   --deploy-mode cluster \
   --name spark-pi \
   --class org.apache.spark.examples.SparkPi \
   --conf spark.executor.instances=5 \
   --conf spark.kubernetes.container.image=<spark-image> \
   local:///path/to/examples.jar
```

ChÃº Ã½ lÃ  `k8s://https://` mÃ¬nh khÃ´ng viáº¿t nháº§m Ä‘Ã¢u nhÃ©.

Nhu cáº§u Ä‘á»ƒ cÃ³ thá»ƒ submit má»™t loáº¡t jobs hÃ ng ngÃ y, team sá»­ dá»¥ng **[Apache Livy](https://livy.incubator.apache.org/)** trÃªn Kubernetes, vá»›i kiáº¿n trÃºc nhÆ° dÆ°á»›i Ä‘Ã¢y:

![](/media/2022/03/spark-k8s-1.png)

Team sá»­ dá»¥ng Livy, Ä‘Ã¢y lÃ  má»™t service cho phÃ©p tÆ°Æ¡ng tÃ¡c vá»›i Spark Cluster thÃ´ng qua RESTful API. Livy Ä‘Ã£ tá»«ng Ä‘Æ°á»£c sá»­ dá»¥ng trÃªn EMR, á»Ÿ Kubernetes chá»‰ cáº§n deploy Livy thÃ´ng qua Helm má»™t cÃ¡ch dá»… dÃ ng. Xem thÃªm cÃ¡ch cÃ i Ä‘áº·t Livy á»Ÿ Ä‘Ã¢y. Äá»ƒ trigger Livy cÃ³ nhiá»u cÃ¡ch, team sá»­ dá»¥ng Airflow nhÆ° lÃ  má»™t scheduler, cÃ³ nhiá»u loáº¡i DAGs tÃ¹y vÃ o tÃ­nh cháº¥t cá»§a má»—i Jobs, cÃ¡c DAG sáº½ trigger Livy, theo dÃµi tráº¡ng thÃ¡i cá»§a Jobs Ä‘Ã³ cÅ©ng thÃ´ng qua API, retry hoáº·c alert khi cáº§n thiáº¿t. DAG cÅ©ng cÃ³ nhiá»‡m vá»¥ kiá»ƒm tra dá»¯ liá»‡u (data validation) káº¿t quáº£ Ä‘áº§u ra (output) cho má»—i jobs.

Tuy nhiÃªn láº¡i cÃ³ má»™t sá»‘ Ä‘iá»ƒm háº¡n cháº¿ nhÆ° do delay tá»« Airflow Scheduler, Livy cÅ©ng dá»… bá»‹ stuck. Náº¿u má»™t jobs cháº¡y lÃ¢u nhÆ°ng Livy bá»‹ restart thÃ¬ Jobs Ä‘Ã³ cÅ©ng bá»‹ áº£nh hÆ°á»Ÿng theo. Team quyáº¿t Ä‘á»‹nh nÃ¢ng cáº¥p.

# 3. Spark on Kubernetes - Spark Operator

Sau khi Ä‘Ã¡nh giÃ¡ kháº£ nÄƒng cá»§a Spark Operator bá»Ÿi GCP Google, team quyáº¿t Ä‘á»‹nh Ä‘i Ä‘áº¿n phiÃªn báº£n 2.0 cá»§a architecture. CÃ¡c thÃ nh pháº§n sáº½ nhÆ° hÃ¬nh dÆ°á»›i Ä‘Ã¢y:

![](/media/2022/03/spark-k8s-2-operator.png)

á» kiáº¿n trÃºc trÃªn, vai trÃ² cá»§a Data Engineer sáº½ lÃ :

- (1) generate ra **Spark Jobs Artifacts** vÃ  commit/push vÃ o má»™t Repo trÃªn Git. Spark Jobs Artifacts sáº½ cÃ³ dáº¡ng nhÆ° vÃ­ dá»¥ nÃ y, cÃ³ thá»ƒ hiá»ƒu Ä‘Ã¢y lÃ  má»™t specs Ä‘á»ƒ Spark Operator cÃ³ thá»ƒ submit vÃ  quáº£n lÃ½ Jobs trÃªn namespace cá»§a mÃ¬nh. CÃ³ 2 loáº¡i CRDs cá»§a Spark Operator sinh ra Ä‘á»ƒ quáº£n lÃ½ lÃ  `SparkApplication` vÃ  `ScheduledSparkApplication`.
- (2) **Spark Submit Worker** lÃ  má»™t Pod cháº¡y trÃªn Kubernetes, cÃ³ nhiá»‡m vá»¥ Ä‘á»c/sync nhá»¯ng gÃ¬ trÃªn Git vÃ  apply vÃ o Kubernetes thÃ´ng qua Kubernetes API (hoáº¡t Ä‘á»™ng giá»‘ng nhÆ° `kubectl apply -f`).
- (3) **Spark Operator** nhÆ° má»i Operator khÃ¡c, sáº½ láº¯ng nghe/Ä‘á»c CRDs Ä‘Æ°á»£c submit vÃ o cluster, sáº½ specify, running, vÃ  cáº­p nháº­t status cá»§a cÃ¡c Spark application. Tá»« má»™t `SparkApplication` Spark Operator sáº½ dá»±ng má»™t POD driver, POD driver sáº½ request thÃªm tá»« Kubernetes Ä‘á»ƒ dá»±ng thÃªm cÃ¡c POD executor, Ä‘áº¿n khi nÃ o Spark Jobs thá»±c hiá»‡n xong sáº½ tá»± Ä‘á»™ng terminate cÃ¡c pod nÃ y. Logs sáº½ Ä‘Æ°á»£c lÆ°u giá»¯ á»Ÿ S3 bucket.
- (4) **Spark History Server** sáº½ render logs tá»« S3 bucket, giÃºp team engineer dá»… dÃ ng hÆ¡n trong viá»‡c traceback láº¡i cÃ¡c jobs cÅ© Ä‘Ã£ finish.
- (5) **Spark Jobs UI** lÃ  má»™t Web UI Ä‘á»ƒ quáº£n lÃ½ táº¥t cáº£ cÃ¡c Spark Jobs trÃªn Git, kiá»ƒm tra tráº¡ng thÃ¡i cá»§a má»—i Jobs trÃªn Cluster, monitor, data validation cÅ©ng nhÆ° backfill.

HÃ£y tÃ¬m hiá»ƒu xem má»™t sá»‘ thÃ nh pháº§n chÃ­nh Ä‘Ã³ng vai trÃ² gÃ¬ nhÃ©.

## 3.1. Spark Operator

Spark Operator lÃ  má»™t Kubernetes Operator Ä‘Æ°á»£c thiáº¿t káº¿ cho Spark nháº±m má»¥c Ä‘Ã­ch xÃ¡c Ä‘á»‹nh vÃ  thá»±c thi cÃ¡c Spark applications dá»… dÃ ng nhÆ° cÃ¡c workloads khÃ¡c trÃªn Kubernetes, báº±ng cÃ¡ch sá»­ dá»¥ng vÃ  quáº£n lÃ½ má»™t Kubernetes custom resources (CRD) Ä‘á»ƒ specifying, running, vÃ  update status cá»§a Spark applications.

Äá»ƒ tÃ¬m hiá»ƒu thÃªm báº¡n cÃ³ thá»ƒ xem qua vá» [Design](https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/design.md), [API Specification](https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/user-guide.md), vÃ  [User Guide](https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/user-guide.md) trÃªn Github.

VÃ­ dá»¥ Ä‘á»ƒ cÃ i Ä‘áº·t Spark Operator trÃªn namespace `spark-jobs` thÃ´ng qua Helm chart, á»Ÿ Ä‘Ã¢y mÃ¬nh báº­t tÃ­nh nÄƒng `webhook`. TÃ¹y vÃ o há»‡ thá»‘ng quáº£n lÃ½ cá»§a báº¡n mÃ  cÃ³ thá»ƒ cÃ i Ä‘áº·t thÃ´ng quan FluxCD hay ArgoCD, á»Ÿ Ä‘Ã¢y mÃ¬nh sá»­ dá»¥ng `helm` cli Ä‘Æ¡n giáº£n cho viá»‡c minh há»a:

```bash
helm repo add spark-operator https://googlecloudplatform.github.io/spark-on-k8s-operator
helm install spark-operator \
   spark-operator/spark-operator \
   --namespace spark-jobs \
   --set sparkJobNamespace=spark-jobs \
   --set webhook.enable=true
```

## 3.2. Spark Submit Worker

Má»™t `SparkApplication` cÃ³ vá» cÆ¡ báº£n lÃ  má»™t resource CRD, cÃ³ thá»ƒ Ä‘Æ°á»£c apply vÃ o cluster báº±ng `kubectl`, nhÆ° vÃ­ dá»¥ dÆ°á»›i Ä‘Ã¢y:

```yaml
# spark-pi.yaml
---
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: pyspark-pi
  namespace: spark-jobs
spec:
  type: Python
  pythonVersion: "3"
  mode: cluster
  image: "gcr.io/spark-operator/spark-py:v3.1.1"
  imagePullPolicy: Always
  mainApplicationFile: local:///opt/spark/examples/src/main/python/pi.py
  sparkVersion: "3.1.1"
  restartPolicy:
    type: OnFailure
    onFailureRetries: 3
    onFailureRetryInterval: 10
    onSubmissionFailureRetries: 5
    onSubmissionFailureRetryInterval: 20
  driver:
    cores: 1
    coreLimit: "1200m"
    memory: "512m"
    labels:
      version: 3.1.1
    serviceAccount: spark
  executor:
    cores: 1
    instances: 1
    memory: "512m"
    labels:
      version: 3.1.1
```

Äá»ƒ submit SparkPi nÃ y vÃ o Kubernetes, báº¡n chá»‰ cáº§n sá»­ dá»¥ng:

```bash
kubectl apply -f spark-pi.yaml
kubectl get sparkapp
```

Äá»ƒ tá»± Ä‘á»™ng hÃ³a, **Spark Submit Worker** lÃ  má»™t Cronjob Pod Ä‘á»ƒ Ä‘á»‹nh ká»³ (~5ph) sync vá»›i Git Repo vÃ  apply má»i spark app dÆ°á»›i dáº¡ng cÃ¡c file YAML vÃ  má»i thay Ä‘á»•i lÃªn cluster. CÃ³ 2 dáº¡ng artifacts mÃ  Spark Submit quáº£n lÃ½ lÃ  `SparkApplication` vÃ  `ScheduledSparkApplication` nhÆ° Ä‘Ã£ nÃ³i á»Ÿ trÃªn.

Viá»‡c quáº£n lÃ½ cÃ¡c Spark Application dÆ°á»›i dáº¡ng YAML specs cÃ²n giÃºp cÃ³ thÃªm má»™t sá»‘ lá»£i Ã­ch cá»§a **GitOps**: má»i thay Ä‘á»•i Ä‘á»u Ä‘Æ°á»£c Git Versioning, phÃ¢n quyá»n trÃªn Git, review thay Ä‘á»•i, approve hoáº·c reject thay Ä‘á»•i, dá»… dÃ ng rollback báº±ng cÃ¡ch revert git, â€¦

## 3.3. Spark Jobs UI

Spark Jobs UI hay Spark Jobs Dashboard lÃ  má»™t Web UI Ä‘á»ƒ quáº£n lÃ½ Spark Jobs vÃ  artifacts Ä‘Æ°á»£c generated hoáº·c customized bá»Ÿi engineers. Dashboard Ä‘Æ°á»£c viáº¿t báº±ng Typescript vÃ  Next.js, gá»“m má»™t sá»‘ tÃ­nh nÄƒng cÆ¡ báº£n nhÆ°:

- Liá»‡t kÃª má»i Spark Jobs artifacts
- Xem ná»™i dung cá»§a tá»«ng Spark Application YAML files
- Xem thÃ´ng tin status cá»§a má»—i Scheduled Spark Application nhÆ° lÃ  `scheduleStatus`, `lastRun`, `nextRun`, ...
- Kiá»ƒm tra nhanh dá»¯ liá»‡u output (basic data validation) cho má»—i jobs theo interval cá»§a Jobs Ä‘Ã³. VÃ­ dá»¥ má»™t jobs theo ngÃ y (daily), UI sáº½ kiá»ƒm tra má»—i ngÃ y xem cÃ³ data cá»§a ngÃ y hÃ´m Ä‘Ã³ cÃ³ há»£p lá»‡ khÃ´ng.
- Thá»‘ng kÃª cÆ¡ báº£n nhÆ° sá»‘ Jobs Ä‘ang cháº¡y, Ä‘ang pending, sá»‘ lÆ°á»£ng Jobs lá»—i, resources (CPU/Memory) sá»­ dá»¥ng, â€¦
- Backfill: cÃ³ thá»ƒ trigger cháº¡y láº¡i cho má»™t hoáº·c nhiá»u jobs, má»™t ngÃ y hoáº·c nhiá»u ngÃ y.

HÃ£y xem má»™t sá»‘ screenshot dÆ°á»›i Ä‘Ã¢y Ä‘á»ƒ cÃ³ cÃ¡i hÃ¬nh cá»¥ thá»ƒ hÆ¡n:

![Fossil Spark UI](/media/2022/03/spark-k8s-3.png)

![Data Validation](/media/2022/03/spark-k8s-4.png)

![Backfill: trigger Ä‘á»ƒ cháº¡y láº¡i Jobs cho má»™t hoáº·c má»™t sá»‘ ngÃ y cá»¥ thá»ƒ trong quÃ¡ khá»©](/media/2022/03/spark-k8s-5.png)

![CÃ³ thá»ƒ custom má»™t sá»‘ config lÃºc cháº¡y backfill, nhÆ° resources, sá»‘ lÆ°á»£ng executor, spark version, ...](/media/2022/03/spark-k8s-6.png)

## 3.4. Spark History Server

Spark History Server lÃ  má»™t Spark Web UI cÃ³ sáºµn cá»§a Spark, dÃ¹ng Ä‘á»ƒ monitor tráº¡ng thÃ¡i vÃ  tÃ i nguyÃªn sá»­ dá»¥ng cho Spark App. Spark History Server Ä‘Æ°á»£c dá»±ng lÃªn Ä‘á»ƒ Ä‘á»c láº¡i logs cá»§a cÃ¡c Jobs Ä‘Ã£ hoÃ n thÃ nh trÆ°á»›c Ä‘Ã³ lÆ°u trÃªn S3 bucket. Má»—i `SparkApplication` sáº½ Ä‘Æ°á»£c config Ä‘á»ƒ push Spark events lÃªn S3:

```yaml
spec:
   sparkConf:
     "spark.eventLog.enabled": "true"
     "spark.eventLog.dir": "s3a://fossil-spark/logs/"
```

Spark History Server cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c cÃ i Ä‘áº·t thÃ´ng qua [this Helm Chart](https://artifacthub.io/packages/helm/spot/spark-history-server), chá»‰ cáº§n trá» Ä‘Ãºng Ä‘Æ°á»ng dáº«n cá»§a `logDirectory` vÃ o Ä‘Ãºng vá»‹ trÃ­ S3 bucket mÃ  Spark Ä‘Ã£ gá»­i lÃªn.

```yaml
helm repo add stable https://kubernetes-charts.storage.googleapis.com
helm install stable/spark-history-server \
 --namespace spark-jobs \
 --set enableS3=true \
 --set logDirectory=s3a://fossil-spark/logs/
```

![Untitled](/media/2022/03/spark-k8s-7.png)

# 4. Performance Tuning on Kubernetes

CÃ³ ráº¥t nhiá»u tá»‘i Æ°u Ä‘Æ°á»£c Ä‘Æ°á»£c thá»±c hiá»‡n do tÃ­nh cháº¥t Spark trÃªn Kubernetes + AWS sáº½ cÃ³ chÃºt khÃ¡c biá»‡t vá»›i Spark trÃªn YARN. 
Má»™t sá»‘ cÃ³ thá»ƒ ká»ƒ Ä‘áº¿n mÃ  báº¡n cÃ³ thá»ƒ xem thÃªm á»Ÿ Ä‘Ã¢y 
[Spark on Kubernetes Performance Tuning](/2021/04/spark-kubernetes-performance-tuning.html) hoáº·c dá»… dÃ ng tÃ¬m kiáº¿m trÃªn Google:

- Using Volcano Scheduler for Gang schedule
- Using Kryo serialization
- Ignoring Data Locality because of S3 data source.
- I/O for S3
- Tuning Java
- Enabled Dynamic Allocation and Dynamic Allocation Shuffle File Tracking
- Using Kubernetes Node Spot instance for the executors.

# 5. Káº¿t

NhÆ° váº­y lÃ  má»i ngÆ°á»i Ä‘Ã£ cÃ³ thá»ƒ hÃ¬nh dung Ä‘Æ°á»£c cÃ¡ch mÃ  team Data Platform táº¡i Fossil sá»­ dá»¥ng váº­n hÃ nh Apache Spark trÃªn Kubernetes.

Do cÃ³ nhiá»u chi tiáº¿t, nhiá»u váº¥n Ä‘á» ká»¹ thuáº­t, cÃ¡ch cÃ i Ä‘áº·t, cÃ¡ch tá»‘i Æ°u, â€¦ mÃ  mÃ¬nh khÃ³ cÃ³ thá»ƒ Ä‘á» cáº­p háº¿t Ä‘Æ°á»£c, do Ä‘Ã³ bÃ i viáº¿t chá»‰ dá»«ng láº¡i á»Ÿ tÃ­nh cháº¥t giá»›i thiá»‡u tá»•ng quÃ¡t. MÃ¬nh sáº½ cá»‘ gáº¯ng chi tiáº¿t hÃ³a cÃ¡c váº¥n Ä‘á» á»Ÿ cÃ¡c bÃ i viáº¿t khÃ¡c náº¿u cÃ³ thá»ƒ trong tÆ°Æ¡ng lai.

<div class="noti">
  BÃ i viáº¿t cÅ©ng Ä‘Æ°á»£c Ä‘Äƒng táº¡i 
  <a href="https://fossil-engineering.github.io/blog/spark-on-kubernetes-at-fossil">Fossil Engineering Blog</a>.
</div>
<div class="noti">
  Hiá»‡n táº¡i Fossil Cloud Data Ä‘ang open cho cÃ¡c vá»‹ trÃ­ (Sr) Data Engineer, 
  <a href="https://sites.google.com/fossil.com/fossil-vietnam/careers/jobs" target="_blank">xem thÃªm JD táº¡i Ä‘Ã¢y</a> 
  hoáº·c gá»­i CV cá»§a báº¡n vá» email <strong>lvduyet (at) fossil.com</strong> Ä‘á»ƒ cÃ¹ng trao Ä‘á»•i thÃªm nhÃ©. 
</div>

# 6. References

- https://kubernetes.io
- https://spark.apache.org
- https://airflow.apache.org
- https://livy.incubator.apache.org
- https://github.com/GoogleCloudPlatform/spark-on-k8s-operator
- https://kubernetes.io/docs/concepts/extend-kubernetes/operator

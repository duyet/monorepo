---
title: NLP - Truyá»‡n Kiá»u Word2vec
date: '2017-04-16'
author: Duyet
tags:
  - Data Engineering
  - Python
  - Javascript
  - Machine Learning
modified_time: '2018-09-10T17:20:37.588+07:00'
thumbnail: https://1.bp.blogspot.com/-O7tdQkuYZ4U/WPOFQVmaFaI/AAAAAAAAkmE/B_LuJ3fxknYsAekzZCy5uOLez3znOiV9wCK4B/s1600/word2vectors.gif
slug: /2017/04/nlp-truyen-kieu-word2vec.html
category: Machine Learning
description: KhÃ¡m phÃ¡ Word2vec qua "Truyá»‡n Kiá»u" cá»§a Nguyá»…n Du. HÆ°á»›ng dáº«n chi tiáº¿t vá» word embeddings, cÃ¡ch xá»­ lÃ½ tiáº¿ng Viá»‡t vá»›i n-grams, vÃ  sá»­ dá»¥ng Gensim Python Ä‘á»ƒ training mÃ´ hÃ¬nh. BÃ i viáº¿t giáº£i thÃ­ch vá» distributed representation, PCA visualization, vÃ  nhá»¯ng phÃ¡t hiá»‡n thÃº vá»‹ vá» má»‘i quan há»‡ ngá»¯ nghÄ©a giá»¯a cÃ¡c tá»« trong kinh Ä‘iá»ƒn vÄƒn há»c Viá»‡t Nam.
---

Trong cÃ¡c dá»± Ã¡n gáº§n Ä‘Ã¢y mÃ¬nh lÃ m nhiá»u vá» Word2vec, khÃ¡ cÃ³ váº» lÃ  useful trong viá»‡c biá»ƒu diá»…n word lÃªn khÃ´ng gian vector (word embedding). NÃ³i thÃªm vá» Word2vec, trong cÃ¡c dá»± Ã¡n nghiÃªn cá»©u W2V cá»§a Google cÃ²n khÃ¡m phÃ¡ Ä‘Æ°á»£c ra tÃ­nh ngá»¯ nghÄ©a, cÃº phÃ¡p cá»§a cÃ¡c tá»« á»Ÿ má»™t sá»‘ má»©c Ä‘á»™ nÃ o Ä‘Ã³. VÃ­ dá»¥ nhÆ° bÃ i toÃ¡n **King + Man - Woman = ?** kinh Ä‘iá»ƒn dÆ°á»›i Ä‘Ã¢y:

![](https://1.bp.blogspot.com/-O7tdQkuYZ4U/WPOFQVmaFaI/AAAAAAAAkmE/B_LuJ3fxknYsAekzZCy5uOLez3znOiV9wCK4B/s1600/word2vectors.gif)

Sá»­ dá»¥ng **word2vec** cho [Truyá»‡n Kiá»u](https://en.wikipedia.org/wiki/The_Tale_of_Kieu) (Nguyá»…n Du), tuy khÃ´ng pháº£i lÃ  má»™t dataset lá»›n, nhÆ°ng biáº¿t Ä‘Ã¢u sáº½ tÃ¬m ra má»‘i liÃªn há»‡ "bÃ­ áº©n" nÃ o Ä‘Ã³ giá»¯a Kiá»u vÃ  MÃ£ GiÃ¡m Sinh. Hay tháº­t sá»± cÃ³ pháº£i chá»¯ **"TÃ i"** Ä‘i vá»›i chá»¯ **"Tai"** nhÆ° Nguyá»…n Du Ä‘Ã£ viáº¿t?

## Word vector lÃ  gÃ¬?

TrÆ°á»›c tiÃªn giá»›i thiá»‡u 1 chÃºt vá» Word vector. Vá» cÆ¡ báº£n, Ä‘Ã¢y chá»‰ lÃ  má»™t vector trá»ng sá»‘, biá»ƒu diá»…n cho 1 tá»«, vá»›i sá»‘ chiá»u cá»¥ thá»ƒ.

VÃ­ dá»¥, 1-of-N (one-hot vector) sáº½ mÃ£ hoÃ¡ (encoding) cÃ¡c tá»« trong tá»« Ä‘iá»ƒn thÃ nh má»™t vector cÃ³ chiá»u dÃ i N (tá»•ng sá»‘ lÆ°á»£ng cÃ¡c tá»« trong tá»« Ä‘iá»ƒn). Giáº£ sá»­ tá»« Ä‘iá»ƒn cá»§a chÃºng ta chá»‰ cÃ³ 5 tá»«: **King, Queen, Man, Woman, vÃ  Child**. Ta cÃ³ thá»ƒ biá»ƒu diá»…n tá»« "Queen" nhÆ° bÃªn dÆ°á»›i:

[![](https://3.bp.blogspot.com/-avTgyW5ipsM/WPOGd7GiNMI/AAAAAAAAkmQ/zMVG_NJ-YOQGs3C4EYlaOt7Dqi-iw4l0wCK4B/s1600/word2vec-one-hot.png)](https://3.bp.blogspot.com/-avTgyW5ipsM/WPOGd7GiNMI/AAAAAAAAkmQ/zMVG_NJ-YOQGs3C4EYlaOt7Dqi-iw4l0wCK4B/s1600/word2vec-one-hot.png)
áº¢nh: blog.acolyer.org

NhÆ°á»£c Ä‘iá»ƒm cá»§a cÃ¡ch biá»ƒu diá»…n nÃ y lÃ  ta khÃ´ng thu Ä‘Æ°á»£c nhiá»u Ã½ nghÄ©a trong viá»‡c so sÃ¡nh cÃ¡c tá»« vá»›i nhau ngoáº¡i trá»« so sÃ¡nh báº±ng, cÃ¡c tá»« cÃ³ Ã½ nghÄ©a hÆ¡n khÃ´ng Ä‘Æ°á»£c nháº¥n máº¡nh.

## Word2vec

NgÆ°á»£c láº¡i, word2vec biá»ƒu diá»…n cÃ¡c tá»« dÆ°á»›i dáº¡ng má»™t phÃ¢n bá»‘ quan há»‡ vá»›i cÃ¡c tá»« cÃ²n láº¡i (distributed representation). Má»—i tá»« Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng má»™t vector cÃ³ cÃ¡c pháº§n tá»­ mang giÃ¡ trá»‹ lÃ  phÃ¢n bá»‘ quan há»‡ cá»§a tá»« nÃ y Ä‘á»‘i vá»›i cÃ¡c tá»« khÃ¡c trong tá»« Ä‘iá»ƒn.

![](https://1.bp.blogspot.com/--U7neeCnIts/WPOG-XnmKYI/AAAAAAAAkmY/w12ZS3LLLqgmNFELDYBMaSnKH-zBa4sQgCK4B/s1600/word2vec-distributed-representation.png)
áº¢nh: blog.acolyer.org

Vá»›i cÃ¡ch biá»ƒu diá»…n nhÆ° váº­y, ngÆ°á»i ta khÃ¡m phÃ¡ ra ráº±ng cÃ¡c vector mang láº¡i cho ta cáº£ cÃº phÃ¡p vÃ  ngá»¯ nghÄ©a á»Ÿ má»™t má»©c Ä‘á»™ nÃ o Ä‘Ã³ Ä‘á»ƒ mÃ¡y tÃ­nh hiá»ƒu.

VÃ­ dá»¥:

- x(apple) â€“ x(apples) â‰ˆ x(car) â€“ x(cars)
- x(family) â€“ x(families) â‰ˆ x(car) â€“ x(cars)

Vá»›i **x(apple)** lÃ  **vector** cá»§a tá»« **_apple_**, ...

![](https://4.bp.blogspot.com/-bAC2VBATSGE/WPOIKvgs-bI/AAAAAAAAkms/Z-JN1kYsAycl8sqXcNUdnh1aAzXZYzzFACK4B/s1600/word2vec-dr-fig-2.png)
áº¢nh: blog.acolyer.org

Báº¡n cÃ³ thá»ƒ tÃ¬m hiá»ƒu ká»¹ hÆ¡n vá» Word2vec **[á»Ÿ bÃ i viáº¿t nÃ y](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/)**.

## Chuáº©n bá»‹ dataset vÃ  tiá»n xá»­ lÃ½

MÃ¬nh tÃ¬m kiáº¿m bá»™ full Truyá»‡n Kiá»u trÃªn Google, lÆ°u vÃ o file **truyen_kieu_data.txt**. Báº¯t Ä‘áº§u tiá»n xá»­ lÃ½.

### 1. Load táº¥t cáº£ cÃ¡c dÃ²ng vÃ o data frame Pandas

<script src="https://gist.github.com/duyet/d7ef8efb03a7e79b61368da3f9a961e8.js"></script>

### 2. Xá»­ lÃ½ tá»«ng dÃ²ng: XÃ³a sá»‘ dÃ²ng, bá» cháº¥m cháº¥m, pháº©y, &nbsp;... cÃ¡c dáº¥u dÆ° thá»«a.

<script src="https://gist.github.com/duyet/bb84042ca6da9b59a32bab7f19bbf8a9.js"></script>

## TÃ¡ch tá»« sá»­ dá»¥ng ngram

Äá»ƒ cho nhanh thay vá» tokenize tiáº¿ng Viá»‡t, mÃ¬nh sáº½ sá»­ dá»¥ng unigram (1-gram) vÃ  bigram (2-gram) Ä‘á»ƒ tÃ¡ch tá»« ra.

Äá»ƒ cho dá»… hiá»ƒu thÃ¬ bigram sáº½ tÃ¡ch cÃ¢u sau thÃ nh:

```python
list(ngrams("TrÄƒm nÄƒm trong cÃµi ngÆ°á»i ta".split(), 2))
> ["TrÄƒm nÄƒm", "nÄƒm trong", "trong cÃµi", "cÃµi ngÆ°á»i", "ngÆ°á»i ta"]
```

Vá»›i cÃ¡c bigram khÃ´ng cÃ³ nghÄ©a nhÆ° _"nÄƒm trong", "trong cÃµi"_. Word2vec cho phÃ©p tham sá»‘ `min_count`, tá»©c nhá»¯ng tá»« nÃ y xuáº¥t hiá»‡n Ã­t hÆ¡n n sáº½ bá»‹ loáº¡i bá», vÃ¬ chÃºng khÃ´ng cÃ³ Ã½ nghÄ©a trong tiáº¿ng Viá»‡t.

<script src="https://gist.github.com/duyet/ee5348c6d449bc90073a827a42d02571.js"></script>

## Combine data vÃ  word2vec vá»›i Gensim Python

Äáº¿n Ä‘oáº¡n hay, truy váº¥n, tÃ¬m nhá»¯ng tá»« gáº§n nhau.

```python
model.wv.similar_by_word("thÃºy kiá»u")

# [('thÃ¢m', 0.9992734789848328),
#  ('bá»¥i', 0.9992575645446777),
#  ('láº½', 0.9992456436157227),
#  ('báº¯t', 0.9992380142211914),
#  ('vÃ­', 0.9992336630821228),
#  ('váº»', 0.9992306232452393),
#  ('sáº¯c', 0.999223530292511),
#  ('dao', 0.9992178678512573),
#  ('phen', 0.9992178082466125),
#  ('hÆ¡n', 0.999215841293335)]
```

\=> "**ThÃ¢m**", Nguyá»…n Du tháº­t thÃ¢m thÃºy khi áº©n dá»¥ vá» **ThÃºy Kiá»u** :))

```python
model.wv.similar_by_word("tÃ i")

# [('thiÃªn', 0.9996418356895447),
#  ('dao', 0.9996212124824524),
#  ('bá»ƒ', 0.9995993375778198),
#  ('muÃ´n', 0.9995784163475037),
#  ('xuÃ´i', 0.9995668530464172),
#  ('00', 0.9995592832565308),
#  ('nÃ¡t', 0.9995554685592651),
#  ('danh', 0.9995527267456055),
#  ('há»a', 0.9995507597923279),
#  ('chá»', 0.9995506405830383)]
```

```python
model.wv.similar_by_word("tÃ¬nh")

# [('phá»¥', 0.9994543790817261),
#  ('Ã¢u', 0.999447226524353),
#  ('nhiÃªu', 0.9994118809700012),
#  ('50', 0.9994039535522461),
#  ('40', 0.9994016885757446),
#  ('cam', 0.9993953108787537),
#  ('quÃ¡', 0.9993941783905029),
#  ('Ä‘á»™ng', 0.9993934631347656),
#  ('nÃ y Ä‘Ã£', 0.999393105506897),
#  ('nhÃ¢n', 0.9993886947631836)]
```

\=> **TÃ¬nh** khÃ´ng **phá»¥** sao gá»i lÃ  tÃ¬nh

```python
model.wv.similar_by_word("Ä‘á»i")

# [('ná»£', 0.9996878504753113),
#  ('thÃ¬ cÅ©ng', 0.9996579885482788),
#  ('nÃ y Ä‘Ã£', 0.9996303915977478),
#  ('miá»‡ng', 0.9996291995048523),
#  ('bÃ¬nh', 0.999627947807312),
#  ('lá»‘i', 0.999624490737915),
#  ('khÃ©o', 0.9996232986450195),
#  ('cÅ©ng lÃ ', 0.999621570110321),
#  ('kia', 0.9996169209480286),
#  ('nhá»', 0.9996155500411987)]
```

**Äá»i** lÃ  má»™t cá»¥c **ná»£** :D

### 5\. PCA vÃ  visualization

PCA giáº£m vector word tá»« 100 chiá»u vá» 2 chiá»u, Ä‘á»ƒ váº½ lÃªn khÃ´ng gian 2 chiá»u.

<script src="https://gist.github.com/duyet/94776c9c4aeb7a18950e6deb799950ee.js"></script>

## Káº¿t

Word2vec chÃ­nh xÃ¡c khi vá»›i bá»™ copus tháº­t lá»›n. Vá»›i vÃ­ dá»¥ trÃªn thá»±c sá»± má»¥c Ä‘Ã­nh chá»‰ lÃ  vui lÃ  chÃ­nh vÃ  Ä‘á»ƒ hiá»ƒu rÃµ hÆ¡n pháº§n nÃ o vá» NLP vá»›i Word2vec. Báº¡n nÃ o cÃ³ há»©ng thÃº cÃ³ thá»ƒ build cÃ¡c bá»™ Word2vec vá»›i dá»¯ liá»‡u cho tiáº¿ng Viá»‡t, vá»›i pháº§n Tokenize vÃ  tiá»n xá»­ lÃ½ chuáº©n - word2vec sáº½ há»¯u Ã­ch ráº¥t nhiá»u.

### Tá»•ng káº¿t nhá»¯ng Ä‘iá»ƒm chÃ­nh

**Word2vec cÆ¡ báº£n:**
- ğŸ”¤ **One-hot encoding** (1-of-N): Biá»ƒu diá»…n Ä‘Æ¡n giáº£n nhÆ°ng khÃ´ng capture Ä‘Æ°á»£c semantic relationship
- ğŸŒ **Distributed representation**: Word2vec biá»ƒu diá»…n tá»« dÆ°á»›i dáº¡ng phÃ¢n bá»‘ quan há»‡ vá»›i cÃ¡c tá»« khÃ¡c
- ğŸ“Š Vector representation cho phÃ©p mÃ¡y tÃ­nh hiá»ƒu cÃº phÃ¡p vÃ  ngá»¯ nghÄ©a á»Ÿ má»©c Ä‘á»™ nháº¥t Ä‘á»‹nh

**Ká»¹ thuáº­t xá»­ lÃ½ tiáº¿ng Viá»‡t:**
- Sá»­ dá»¥ng **unigram (1-gram)** vÃ  **bigram (2-gram)** cho tokenization nhanh
- Tham sá»‘ `min_count` loáº¡i bá» cÃ¡c tá»« xuáº¥t hiá»‡n Ã­t (nhÆ° "nÄƒm trong", "trong cÃµi")
- Tiá»n xá»­ lÃ½: XÃ³a sá»‘ dÃ²ng, kÃ½ tá»± Ä‘áº·c biá»‡t, dáº¥u cÃ¢u thá»«a

**CÃ´ng cá»¥ vÃ  thÆ° viá»‡n:**
- ğŸ **Pandas** - Load vÃ  xá»­ lÃ½ data
- ğŸ“š **NLTK ngrams** - TÃ¡ch tá»« vá»›i n-gram
- ğŸ”§ **Gensim** - Training word2vec model
- ğŸ“‰ **PCA** - Giáº£m chiá»u Ä‘á»ƒ visualization

**Nhá»¯ng phÃ¡t hiá»‡n thÃº vá»‹ tá»« Truyá»‡n Kiá»u:**
```
"thÃºy kiá»u" â†’ "thÃ¢m" (ThÃ¢m thÃºy!)
"tÃ i" â†’ "thiÃªn", "dao", "bá»ƒ" (TÃ i cao vÃºt nhÆ° trá»i biá»ƒn)
"tÃ¬nh" â†’ "phá»¥" (TÃ¬nh khÃ´ng phá»¥ sao gá»i lÃ  tÃ¬nh)
"Ä‘á»i" â†’ "ná»£" (Äá»i lÃ  má»™t cá»¥c ná»£)
```

**Háº¡n cháº¿ vÃ  cáº£i thiá»‡n:**
- âš ï¸ Dataset nhá» (chá»‰ Truyá»‡n Kiá»u) â†’ Káº¿t quáº£ mang tÃ­nh giáº£i trÃ­ > thá»±c táº¿
- ğŸ’¡ Äá»ƒ chÃ­nh xÃ¡c hÆ¡n cáº§n:
  - Corpus lá»›n hÆ¡n nhiá»u (Wikipedia tiáº¿ng Viá»‡t, bÃ¡o chÃ­, sÃ¡ch,...)
  - Tokenizer chuáº©n cho tiáº¿ng Viá»‡t (VnCoreNLP, underthesea, pyvi)
  - Fine-tuning hyperparameters (vector_size, window, min_count,...)
  - Training time dÃ i hÆ¡n vá»›i nhiá»u epochs

**á»¨ng dá»¥ng thá»±c táº¿:**
- ğŸ“ Sentiment analysis
- ğŸ” Information retrieval vÃ  search
- ğŸ¤– Chatbots vÃ  virtual assistants
- ğŸ“– Document classification
- ğŸ”„ Machine translation
- âœï¸ Text generation

Tham kháº£o thÃªm:

- Truyá»‡n Kiá»u Word2vec at Github: [https://github.com/duyet/truyenkieu-word2vec](https://github.com/duyet/truyenkieu-word2vec)
- [The amazing power of word vectors](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/)
- [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf) â€“ Mikolov et al. 2013
- [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) â€“ Mikolov et al. 2013
- [Vietnamese NLP Toolkit - underthesea](https://github.com/undertheseanlp/underthesea)
- [VnCoreNLP](https://github.com/vncorenlp/VnCoreNLP)
